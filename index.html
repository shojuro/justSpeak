<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TalkTime - Practice English Conversation</title>
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#2C2C2E">
    <style>
        :root {
            --warm-coral: #FF6B7A;
            --warm-coral-light: #FFB3BA;
            --deep-charcoal: #2C2C2E;
            --deep-charcoal-light: #3A3A3C;
            --sage-green: #87A96B;
            --soft-gold: #F4D03F;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(to bottom, var(--deep-charcoal), var(--deep-charcoal-light));
            color: white;
            height: 100vh;
            overflow: hidden;
            margin: 0;
        }

        .screen {
            display: none;
            height: 100vh;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 1.5rem;
        }

        .screen.active {
            display: flex;
        }

        .logo {
            font-size: 3rem;
            font-weight: bold;
            margin-bottom: 1rem;
        }

        .logo .talk {
            color: var(--warm-coral);
        }

        .logo .time {
            font-weight: 300;
        }

        .subtitle {
            font-size: 1.125rem;
            opacity: 0.8;
            margin-bottom: 3rem;
            text-align: center;
            max-width: 400px;
        }

        .btn {
            background: var(--warm-coral);
            color: white;
            border: none;
            padding: 1rem 2rem;
            font-size: 1rem;
            font-weight: 600;
            border-radius: 9999px;
            cursor: pointer;
            transition: all 0.3s;
            min-width: 200px;
        }

        .btn:hover {
            background: var(--warm-coral-light);
            transform: scale(1.05);
        }

        .btn:disabled {
            background: #666;
            cursor: not-allowed;
            transform: scale(1);
        }

        .conversation-container {
            height: 100vh;
            width: 100%;
            max-width: 600px;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .header {
            flex-shrink: 0;
            padding: 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .messages {
            flex: 1;
            overflow-y: auto;
            padding: 1rem;
            display: flex;
            flex-direction: column;
            gap: 1rem;
            min-height: 0;
        }

        .message {
            max-width: 80%;
            padding: 1rem;
            border-radius: 1rem;
            word-wrap: break-word;
        }

        .message.user {
            align-self: flex-end;
            background: var(--warm-coral);
        }

        .message.assistant {
            align-self: flex-start;
            background: rgba(255, 255, 255, 0.1);
        }

        .controls {
            flex-shrink: 0;
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 1rem;
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            background: linear-gradient(to top, var(--deep-charcoal-light), transparent);
        }

        .mic-button {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            border: none;
            background: var(--sage-green);
            color: white;
            cursor: pointer;
            transition: all 0.3s;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .mic-button:hover {
            transform: scale(1.05);
        }

        .mic-button.listening {
            background: var(--warm-coral);
            animation: pulse 1.5s infinite;
        }

        .mic-button:disabled {
            background: #666;
            cursor: not-allowed;
        }

        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(255, 107, 122, 0.4); }
            70% { box-shadow: 0 0 0 20px rgba(255, 107, 122, 0); }
            100% { box-shadow: 0 0 0 0 rgba(255, 107, 122, 0); }
        }

        .status {
            font-size: 0.875rem;
            opacity: 0.6;
        }

        .success-icon {
            width: 80px;
            height: 80px;
            color: var(--soft-gold);
            margin-bottom: 2rem;
        }

        .btn-secondary {
            background: rgba(255, 255, 255, 0.1);
            margin-top: 1rem;
        }

        .btn-secondary:hover {
            background: rgba(255, 255, 255, 0.2);
        }

        .sound-waves {
            margin-top: 2rem;
            animation: pulse 2s infinite;
        }
    </style>
</head>
<body>
    <!-- Welcome Screen -->
    <div id="welcome" class="screen active">
        <div class="logo">
            <span class="talk">Talk</span><span class="time">Time</span>
        </div>
        <p class="subtitle">Practice English conversation with your friendly AI partner</p>
        <button class="btn" onclick="startConversation()">Start Talking</button>
        <svg class="sound-waves" width="100" height="100" viewBox="0 0 100 100">
            <circle cx="50" cy="50" r="45" fill="none" stroke="currentColor" stroke-width="2" opacity="0.3" />
            <circle cx="50" cy="50" r="35" fill="none" stroke="currentColor" stroke-width="2" opacity="0.5" />
            <circle cx="50" cy="50" r="25" fill="none" stroke="currentColor" stroke-width="2" opacity="0.7" />
        </svg>
    </div>

    <!-- Conversation Screen -->
    <div id="conversation" class="screen">
        <div class="conversation-container">
            <div class="header">
                <h2>TalkTime</h2>
                <button onclick="endSession()" style="background: none; border: none; color: white; opacity: 0.6; cursor: pointer;">End Session</button>
            </div>
            <div id="messages" class="messages"></div>
            <div class="controls">
                <button id="micButton" class="mic-button" onclick="toggleListening()">
                    <svg width="32" height="32" fill="currentColor" viewBox="0 0 24 24">
                        <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z" />
                        <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z" />
                    </svg>
                </button>
                <p id="status" class="status">Tap to speak</p>
            </div>
        </div>
    </div>

    <!-- Session Complete Screen -->
    <div id="complete" class="screen">
        <svg class="success-icon" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z" />
        </svg>
        <h2 style="font-size: 2rem; margin-bottom: 1rem;">Great conversation!</h2>
        <p class="subtitle">You're doing amazing. Every conversation helps you get more comfortable speaking English.</p>
        <button class="btn" onclick="continueConversation()">Continue Talking</button>
        <button class="btn btn-secondary" onclick="newSession()">End Session</button>
    </div>

    <script>
        let recognition;
        let synthesis = window.speechSynthesis;
        let isListening = false;
        let messages = [];
        let sessionId = null;
        let speakingStartTime = null;
        let aiSpeakingStartTime = null;

        // Initialize session
        async function initSession() {
            try {
                const response = await fetch('/api/session/start', { method: 'POST' });
                const data = await response.json();
                sessionId = data.sessionId;
                console.log('Session started:', sessionId);
            } catch (error) {
                console.error('Failed to start session:', error);
            }
        }

        // Track speaking time
        async function trackSpeakingTime(duration, speaker) {
            if (!sessionId) return;
            
            try {
                await fetch('/api/session/update', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        sessionId,
                        speakingTime: duration,
                        speaker
                    })
                });
            } catch (error) {
                console.error('Failed to track speaking time:', error);
            }
        }

        // Initialize speech recognition
        if ('webkitSpeechRecognition' in window) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                isListening = true;
                speakingStartTime = Date.now();
                document.getElementById('micButton').classList.add('listening');
                document.getElementById('status').textContent = 'Listening...';
            };

            recognition.onresult = (event) => {
                let finalTranscript = '';
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    }
                }
                // Only process if we have meaningful content
                if (finalTranscript && finalTranscript.trim().length > 0) {
                    recognition.stop(); // Stop listening after getting user input
                    addMessage('user', finalTranscript.trim());
                    respondToUser(finalTranscript.trim());
                }
            };

            recognition.onend = () => {
                isListening = false;
                if (speakingStartTime) {
                    const duration = (Date.now() - speakingStartTime) / 1000;
                    trackSpeakingTime(duration, 'user');
                    speakingStartTime = null;
                }
                document.getElementById('micButton').classList.remove('listening');
                document.getElementById('status').textContent = 'Tap to speak';
            };
        }

        function showScreen(screenId) {
            document.querySelectorAll('.screen').forEach(screen => {
                screen.classList.remove('active');
            });
            document.getElementById(screenId).classList.add('active');
        }

        async function startConversation() {
            showScreen('conversation');
            messages = [];
            await initSession();
            addMessage('assistant', "Hi! I'm excited to chat with you. What would you like to talk about today?");
            speak("Hi! I'm excited to chat with you. What would you like to talk about today?");
        }

        function endSession() {
            if (recognition && isListening) {
                recognition.stop();
            }
            showScreen('complete');
        }

        function continueConversation() {
            showScreen('conversation');
        }

        function newSession() {
            showScreen('welcome');
        }

        function toggleListening() {
            if (!recognition) {
                alert('Speech recognition is not supported in your browser.');
                return;
            }

            if (isListening) {
                recognition.stop();
            } else if (!synthesis.speaking) {
                recognition.start();
            }
        }

        function addMessage(role, content) {
            const messagesDiv = document.getElementById('messages');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            messageDiv.textContent = content;
            messagesDiv.appendChild(messageDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
            messages.push({ role, content });
        }

        let currentAudio = null;
        
        async function speak(text) {
            // Update UI
            document.getElementById('micButton').disabled = true;
            document.getElementById('status').textContent = 'AI is speaking...';
            
            try {
                // Try ElevenLabs first
                const response = await fetch('/api/speech', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        text: text,
                        voiceId: 'pFZP5JQG7iQjIQuC4Bku' // Hope's voice
                    })
                });
                
                if (response.ok) {
                    const audioBlob = await response.blob();
                    const audioUrl = URL.createObjectURL(audioBlob);
                    
                    currentAudio = new Audio(audioUrl);
                    aiSpeakingStartTime = Date.now();
                    
                    currentAudio.onended = () => {
                        if (aiSpeakingStartTime) {
                            const duration = (Date.now() - aiSpeakingStartTime) / 1000;
                            trackSpeakingTime(duration, 'ai');
                            aiSpeakingStartTime = null;
                        }
                        document.getElementById('micButton').disabled = false;
                        document.getElementById('status').textContent = 'Tap to speak';
                        URL.revokeObjectURL(audioUrl);
                    };
                    
                    await currentAudio.play();
                } else {
                    throw new Error('ElevenLabs failed, falling back to browser TTS');
                }
            } catch (error) {
                console.log('Falling back to browser TTS:', error.message);
                // Fallback to browser TTS
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'en-US';
                utterance.rate = 0.9;
                
                utterance.onstart = () => {
                    aiSpeakingStartTime = Date.now();
                    document.getElementById('micButton').disabled = true;
                    document.getElementById('status').textContent = 'AI is speaking...';
                };
                
                utterance.onend = () => {
                    if (aiSpeakingStartTime) {
                        const duration = (Date.now() - aiSpeakingStartTime) / 1000;
                        trackSpeakingTime(duration, 'ai');
                        aiSpeakingStartTime = null;
                    }
                    document.getElementById('micButton').disabled = false;
                    document.getElementById('status').textContent = 'Tap to speak';
                };
                
                synthesis.speak(utterance);
            }
        }

        async function respondToUser(userText) {
            // Show thinking indicator
            document.getElementById('status').textContent = 'AI is thinking...';
            document.getElementById('micButton').disabled = true;
            
            try {
                // Build conversation context (last 10 messages)
                const context = messages.slice(-10).map(msg => ({
                    role: msg.role,
                    content: msg.content
                }));

                // Call API
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: userText,
                        context: context,
                        ageGroup: 'adult', // Could be made dynamic based on user selection
                        sessionId: sessionId
                    })
                });

                if (!response.ok) {
                    throw new Error('Failed to get response');
                }

                const data = await response.json();
                const aiReply = data.reply;

                // Add AI message and speak it
                addMessage('assistant', aiReply);
                speak(aiReply);

            } catch (error) {
                console.error('Error getting AI response:', error);
                
                // Fallback response
                const fallbackResponses = [
                    "I'm having trouble connecting right now. Can you tell me more about that?",
                    "Sorry, I couldn't quite process that. What were you saying?",
                    "Let me think about that differently. Could you share more details?"
                ];
                
                const fallback = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)];
                addMessage('assistant', fallback);
                speak(fallback);
            } finally {
                // Re-enable microphone after speaking starts
                setTimeout(() => {
                    if (!synthesis.speaking) {
                        document.getElementById('micButton').disabled = false;
                        document.getElementById('status').textContent = 'Tap to speak';
                    }
                }, 500);
            }
        }

        // Register service worker
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.register('/service-worker.js');
        }
    </script>
</body>
</html>